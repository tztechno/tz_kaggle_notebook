{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## [1. Import Needed Modules](#import) ##\n## [2. ASK Callback Definition](#ask) ## \n## [3. SLR Callback Definition](#slr) ##\n## [4. SOMT Callback Definition](#somt) ## \n## [5. TLC Callback Definition](#tlc) ## \n## [6. SPREADSHEET Callback Definition](#spreadsheet) ## \n## [7. DWELL Callback Definition](#dwell) ## \n## [8. LRA Callback Definition](#lra) ## \n## [9. Code to Setup Callback Demonstrations](#setup) ## \n## [10. Demonstration of ASK Callback](#askdemo) ## \n## [11. Demonstration of SLR Callback](#slrdemo) ##\n## [12. Demonstration of SOMT Callback](#somtdemo) ##\n## [13. Demonstration of TLC Callback](#tlcdemo) ##\n## [14. Demonstration of SPREADSHEET Callback](#spreadsheetdemo) ##\n## [15. Demonstration of DWELL Callback](#dwelldemo) ##\n## [16. Demonstration of DWELL Callback](#dwelldemo) ##","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\n# <center>Import Need Modules</center>","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model\nimport numpy as np\nimport time\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nprint ('modules loaded')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:02.768030Z","iopub.execute_input":"2022-01-23T17:51:02.769071Z","iopub.status.idle":"2022-01-23T17:51:02.788342Z","shell.execute_reply.started":"2022-01-23T17:51:02.769021Z","shell.execute_reply":"2022-01-23T17:51:02.787591Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"modules loaded\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Below are a series of custom callbacks I have found to be very  useful when training a model.  \n## The use of these callbacks is demonstrated in the code below. To demonstrate I use this dataset\n## with a reduced image size and a smaller number of samples per class to speed up training time\n## for the 7 different callbaack demos, Object here is to demo the callbacks and not necessarily\n## achieve a high accuracy","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ask\"></a>\n# <center>ASK Callback Definition</center>","metadata":{}},{"cell_type":"markdown","source":"### The ASK callback is useful when training a model and based on model performance metrics you can elect to halt or continue training.\nOften when you first run a model you are not sure of how man epochs to run. For example you may initially specify 5 epochs in\nmodel.fit and your model is training well so you would really like to run more epochs but to do that you have to start over\nand set epochs to say 15. Conversely you may specify epochs as 100 and your model performance maxes out metric wise at around\n10 epochs. So now you have t0 kill the kernel and rerun everything again with epochs set to 15. The ASK callbacks enables you\nto easily continue training for an additional number of epochs or halt training. The form of use of the callback is\ncallbacks=[ASK(model, epochs, ask_epoch)] where\n* model is the name of your compiled model\n* epochs is the number of epochs you specified in model.fit\n* ask_epoch is an integer. Assume it is set to a value N. When the model is training when it completes the Nth epoch you\n  will receive a printed query asking if you wish to halt train by entering an H, or to enter an integer say for example 5.\n  In that case training will continue up to epoch N+5 at which time you will be queried again. Note however training will\n  always end when epochs number of epochs has been run. Typically make epochs a large number like 100 so it does not end training\n  before you elect to do so.","metadata":{}},{"cell_type":"code","source":"class ASK(keras.callbacks.Callback):\n    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n        super(ASK, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True # if True query the user on a specified epoch\n        \n    def on_train_begin(self, logs=None): # this runs on the beginning of training\n        if self.ask_epoch == 0: \n            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n            self.ask=False # do not query the user\n        if self.epochs == 1:\n            self.ask=False # running only for 1 epoch so do not query user\n        else:\n            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')  \n        self.start_time= time.time() # set the time at which training started\n        \n    def on_train_end(self, logs=None):   # runs at the end of training     \n        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n        hours = tr_duration // 3600\n        minutes = (tr_duration - (hours * 3600)) // 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) # print out training duration time\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask: # are the conditions right to query the user?\n            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n                ans=input()\n                \n                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True # halt training\n                else: # user wants to continue training\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n                    else:\n                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:02.791578Z","iopub.execute_input":"2022-01-23T17:51:02.793029Z","iopub.status.idle":"2022-01-23T17:51:02.823839Z","shell.execute_reply.started":"2022-01-23T17:51:02.792991Z","shell.execute_reply":"2022-01-23T17:51:02.823191Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"slr\"></a>\n# <center>SLR Callback Definition</center>","metadata":{}},{"cell_type":"markdown","source":"### The SLR callback is useful if you wish to change the learning rate during training\nThe callback is a modification of the ASK callback above where when queired you can specify a new learning rate.  \nThe form of use of the callback is callbacks=[SLR(model, epochs, ask_epoch)] where\n* model is the name of your compiled model\n* epochs is the number of epochs you specified in model.fit\n* ask_epoch is an integer. Assume it is set to a value N. When the model is training when it completes the Nth epoch you\n  will receive a printed query asking if you wish to halt train by entering an H, or to enter an integer say for example 5\n  to continue training for 5 more epochs using the current learning rate, or enter A to adjusts the learning rate. If you\n  enter A you will be queired to enter a float value as the new learning rate. After entering the new learning rate you are\n  asked to enter an integer for the number of additional epochs to run then be queired again.","metadata":{}},{"cell_type":"code","source":"class SLR(keras.callbacks.Callback):\n    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n        super(SLR, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True # if True query the user on a specified epoch\n        \n    def on_train_begin(self, logs=None): # this runs on the beginning of training\n        if self.ask_epoch == 0: \n            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n            self.ask=False # do not query the user\n        if self.epochs == 1:\n            self.ask=False # running only for 1 epoch so do not query user\n        else:\n            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n            print(' enter H to halt training, or enter an integer for how many more epochs to run, then be asked again')  \n            print(' or enter an A to adjust the learning rate. If an A is entered you will be queired to enter a float')\n            print(' values for the new learning rate then be asked to enter an integer for how many more epochs to run before being asked again')\n        self.start_time= time.time() # set the time at which training started\n        \n    def on_train_end(self, logs=None):   # runs at the end of training     \n        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n        hours = tr_duration // 3600\n        minutes = (tr_duration - (hours * 3600)) // 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) # print out training duration time\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask: # are the conditions right to query the user?\n            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n                print('\\n Enter H to end training or an integer for the number of additional epochs to run or enter A to adjust the learning rate')\n                ans=input()                \n                if ans == 'H' or ans =='h' or ans == '0': # halt training \n                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True # halt training\n                elif ans == 'A' or ans == 'a': # user wants to adjust learning rate\n                    lr=float(tf.keras.backend.get_value(self.model.optimizer.lr))\n                    print(' current lr = ', lr, 'Enter a float value for the new learning rate')\n                    ans=input()\n                    lr=float(ans)\n                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n                    print('Enter an integer for the number of additional epochs to run then be asked again')\n                    ans=input()\n                    self.ask_epoch += int (ans)\n                    print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)\n                else: # user wants to continue training with current lr\n                    self.ask_epoch += int(ans)\n                    print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:02.828304Z","iopub.execute_input":"2022-01-23T17:51:02.830723Z","iopub.status.idle":"2022-01-23T17:51:02.856278Z","shell.execute_reply.started":"2022-01-23T17:51:02.830683Z","shell.execute_reply":"2022-01-23T17:51:02.852597Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"somt\"></a>\n# <center>SOMT Callback Definition</center>","metadata":{}},{"cell_type":"markdown","source":"### The SOMT callback is useful to end training based on the value of the training accuracy or the validation loss or both.\nThe form of use is callbacks=[SOMT(model, train_thold, valid_thold)] where\n\n* model is the name of your complied model\n* train_thold is a float. It is the value of accuracy (in Percent) that must be achieved by the model in order to conditionally stop training\n* valid_threshold is a float. It is the value of validation accuracy (in Percent) that must be achieved by the model\n  in order to conditionally stop training\n\nNote to stop training BOTH the train_thold and valid_thold must be exceeded in the SAME epoch.   \nIf you want to stop training based soley on the training accuracy set the valid_thold to 0.0.  \nSimilarly if you want to stop training on just validation accuracy set train_thold= 0.0.   \nNote if both thresholds are not achieved in the same epoch training will continue until the value of epochs  \nset in model.fit is reached. For example lets take the case that you want to stop training when the   \ntraining accuracy has reached or exceeded 95 % and the validation accuracy has achieved at least 85%   \nthen the code would be callbacks=[SOMT(my_model, .95, .85)]","metadata":{}},{"cell_type":"code","source":"class SOMT(keras.callbacks.Callback):\n    def __init__(self, model,  train_thold, valid_thold):\n        super(SOMT, self).__init__()\n        self.model=model        \n        self.train_thold=train_thold\n        self.valid_thold=valid_thold\n        \n    def on_train_begin(self, logs=None):\n        print('Starting Training - training will halt if training accuracy achieves or exceeds ', self.train_thold)\n        print ('and validation accuracy meets or exceeds ', self.valid_thold) \n        msg='{0:^8s}{1:^12s}{2:^12s}{3:^12s}{4:^12s}{5:^12s}'.format('Epoch', 'Train Acc', 'Train Loss','Valid Acc','Valid_Loss','Duration')\n        print (msg)                                                                                    \n            \n    def on_train_batch_end(self, batch, logs=None):\n        acc=logs.get('accuracy')* 100  # get training accuracy \n        loss=logs.get('loss')\n        msg='{0:1s}processed batch {1:4s}  training accuracy= {2:8.3f}  loss: {3:8.5f}'.format(' ', str(batch),  acc, loss)\n        print(msg, '\\r', end='') # prints over on the same line to show running batch count \n        \n    def on_epoch_begin(self,epoch, logs=None):\n        self.now= time.time()\n        \n    def on_epoch_end(self,epoch, logs=None): \n        later=time.time()\n        duration=later-self.now \n        tacc=logs.get('accuracy')           \n        vacc=logs.get('val_accuracy')\n        tr_loss=logs.get('loss')\n        v_loss=logs.get('val_loss')\n        ep=epoch+1\n        print(f'{ep:^8.0f} {tacc:^12.2f}{tr_loss:^12.4f}{vacc:^12.2f}{v_loss:^12.4f}{duration:^12.2f}')\n        if tacc>= self.train_thold and vacc>= self.valid_thold:\n            print( f'\\ntraining accuracy and validation accuracy reached the thresholds on epoch {epoch + 1}' )\n            self.model.stop_training = True # stop training","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:02.858249Z","iopub.execute_input":"2022-01-23T17:51:02.858571Z","iopub.status.idle":"2022-01-23T17:51:02.876702Z","shell.execute_reply.started":"2022-01-23T17:51:02.858537Z","shell.execute_reply":"2022-01-23T17:51:02.875919Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"tlc\"></a>\n# <center>TLC Callback Definition</center>","metadata":{}},{"cell_type":"markdown","source":"The TLC callback is a variation of the ASK callback that is useful in the case where you are doing transfer learning.   \nThe form of use is callbacks=[TLC(model, base_model,epochs, ask_epoch)] where\n* model is the name of your compiled model\n* base_model is the name of the base model providing transfer learning. For example in the code below I create a model using the\n  EfficientNetB3 as the base_model. \n   * base_model=tf.keras.applications.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n   * base_model.trainable= False    \n   * x=base_model.output \n   * x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n   * x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n     bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n   * x=Dropout(rate=.45, seed=123)(x)\n   * output=Dense(class_count, activation='softmax')(x) \n   * my_model=Model(inputs=base_model.input, outputs=output) \n   * my_model.compile(Adamax(lr=.005), loss='categorical_crossentropy', metrics=['accuracy'])\n     Note make sure you include base_model.trainable=False so that when queried you can provide an input T to make the base_model trainable when asked\n  \n  \n * ask_epoch is an integer. Lets it is set to 5. In that case at the end of the 5th epoch you will be queried to enter either  \n   T to make the base_model trainable or enter an integer say 4. In that case at the end of the 9th epoch you will be queried again.  \n   If you enter T to make the base_model trainable you will then be asked to enter an integer N say you enter 3.   \n   Then you will train the model for 3 more epochs then be asked to either enter H to halt training or enter an integer  \n   for how many more epochs to run then be queried again.  \n * epochs is the value of epochs you used in model.fit","metadata":{}},{"cell_type":"code","source":"class TLC(keras.callbacks.Callback):\n    def __init__(self, model, base_model, epochs,  ask_epoch):\n        super(TLC, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True\n        self.base_model=base_model        \n        \n    def on_train_begin(self, logs=None):        \n        if self.ask_epoch == 0:\n            self.ask=False\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs:\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs' )           \n            self.ask=False\n        if self.epochs == 1:\n            self.ask=False\n        else:\n            if self.base_model !=None and self.base_model.trainable == False:                \n                msg=f'Training will proceed until epoch {ask_epoch}, then you will be asked to either halt, continue \\n'\n                msg=msg + 'or make the base model trainable then asked to enter number of epochs to run then ask again\\n'\n                print (msg, flush=True)\n            else:\n                msg=f'Training will proceed until epoch {ask_epoch}, then you will be asked to either halt \\n'\n                msg=msg + 'or enter the number of epochs to train on then you will be asked again\\n'\n                print (msg, flush=True)\n            \n        self.start_time= time.time()\n        \n    def on_train_end(self, logs=None):        \n        tr_duration=time.time() - self.start_time            \n        hours = tr_duration // 3600\n        minutes = (tr_duration - (hours * 3600)) // 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) \n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask:\n            if epoch + 1 ==self.ask_epoch:                \n                if self.base_model !=None and self.base_model.trainable ==  False:\n                    msg='\\n Enter H to end training,an integer for the number of additional epochs to run then ask again\\n'\n                    msg=msg + 'or T to train base model\\n'                    \n                else:\n                    msg='\\n Enter H to end training,an integer for the number of additional epochs to run then ask again\\n'\n                ans=input(msg )\n                if ans == 'T' or ans== 't':\n                    self.base_model.trainable=True\n                    print ('Base Model trainable is now set to ', self.base_model.trainable)\n                    msg='\\n enter an integer for the number of additional epochs to run then ask again\\n'\n                    ans=(input(msg))\n                    \n                if ans == 'H' or ans =='h' or ans == '0':\n                    print ('\\nTraining halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True\n                else:\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush=True)\n                    else:\n                        print ('\\n Training will continue to epoch ', self.ask_epoch, flush=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:02.881240Z","iopub.execute_input":"2022-01-23T17:51:02.883391Z","iopub.status.idle":"2022-01-23T17:51:02.906067Z","shell.execute_reply.started":"2022-01-23T17:51:02.883353Z","shell.execute_reply":"2022-01-23T17:51:02.905219Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"spreadsheet\"></a>\n# <center>SPREADSHEET Callback Definition</center>","metadata":{}},{"cell_type":"markdown","source":"The SPREADSHEET callback formats the printout used during training in a spreadsheet style. It has the same fuctionality  \nas the TLC callback but prints out the training data in the example format shown below.  \nThe form of use is  callbacks=[SPREADSHEET(model, base_model, epochs, ask_epoch, batches)] where  \n* model is the name of your compiled model\n* base_model is the name of the base model providing transfer learning.  \n  If you are not doing transfer learning set base_model=None.  \n  For example in the code below I create a model using the EfficientNetB3 as the base_model.   \n   * base_model=tf.keras.applications.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n   * base_model.trainable= False    \n   * x=base_model.output \n   * x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n   * x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n     bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n   * x=Dropout(rate=.45, seed=123)(x)\n   * output=Dense(class_count, activation='softmax')(x) \n   * my_model=Model(inputs=base_model.input, outputs=output) \n   * my_model.compile(Adamax(lr=.005), loss='categorical_crossentropy', metrics=['accuracy'])\n     Note make sure you include base_model.trainable=False so that when queried you can provide an input T to make the base_model trainable when asked\n  \n  \n * ask_epoch is an integer. Lets say it is set to 5. In that case at the end of the 5th epoch  \n   you will be queried to enter either T to make the base_model trainable or enter an integer say 4.  \n   In that case at the end of the 9th epoch you will be queried again.  \n   If you enter T to make the base_model trainable you will then be asked to enter an integer N say you enter 3.   \n   Then you will train the model for 3 more epochs then be asked to either enter H to halt training or enter an integer  \n   for how many more epochs to run then be queried again.\n * epochs is the value of epochs you used in model.fit\n * batches is an integer that defines the number of batches that are run per epoch.   \n   This can be calculated as the number of samples in the training set divided by the batch size.  \n   Use batches=np.celing(training samples/batch_size). For example if your training  \n   set has say 1,000 images and a batch size of 50, then batches would be 20.\n  \n","metadata":{}},{"cell_type":"code","source":"class SPREADSHEET(keras.callbacks.Callback):\n    def __init__(self, model, base_model, epochs,  ask_epoch, batches):\n        super(SPREADSHEET, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True\n        self.base_model=base_model \n        self.lowest_vloss=np.inf # set lowest validation loss to infinity initially\n        self.batches=batches        \n        \n    def on_train_begin(self, logs=None): \n        if self.ask_epoch == 0:\n            self.ask=False            \n        if self.ask_epoch >= self.epochs:\n            msg=f'ask_epoch >= epochs, will train for {epochs}  epochs'\n            print_in_color(msg, (0,255,255), (55,65,80))\n            self.ask=False\n        if self.epochs == 1:\n            self.ask=False\n        if self.ask == True:\n            if self.base_model !=None and self.base_model.trainable == False:                \n                msg=f'Training will proceed until epoch {ask_epoch}, then you will be asked to either halt, continue \\n'\n                msg=msg + 'or make the base model trainable then asked to enter number of epochs to run then ask again\\n'\n                print_in_color (msg, (0,255,255), (55,65,80))\n            else:\n                msg=f'Training will proceed until epoch {ask_epoch}, then you will be asked to either halt \\n'\n                msg=msg + 'or enter the number of epochs to train on then you will be asked again\\n'\n                print_in_color (msg, (0,255,255), (55,65,80))\n            \n        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:10s}{7:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', '% Improv', 'Duration')\n        print_in_color(msg, (244,252,3), (55,65,80)) \n        self.start_time= time.time()\n        \n    def on_train_end(self, logs=None):        \n        tr_duration=time.time() - self.start_time            \n        hours = tr_duration // 3600\n        minutes = (tr_duration - (hours * 3600)) // 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print_in_color (msg, (0,255,255), (55,65,80)) \n        \n    def on_train_batch_end(self, batch, logs=None):\n        acc=logs.get('accuracy')* 100  # get training accuracy \n        loss=logs.get('loss')\n        msg='{0:20s}processing batch {1:4s} of {2:5s} accuracy= {3:8.3f}  loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n        print(msg, '\\r', end='') # prints over on the same line to show running batch count        \n        \n    def on_epoch_begin(self,epoch, logs=None):\n        self.now= time.time()\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        later=time.time()\n        color= (0,255,0)\n        duration=later-self.now \n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        current_lr=lr\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        if epoch ==0:\n                pimprov=0.0\n                self.lowest_vloss=v_loss\n        else:\n            pimprov=( self.lowest_vloss- v_loss) * 100/self.lowest_vloss\n            if v_loss > self.lowest_vloss:\n                color=(245, 170, 66)\n            else:\n                color=(0,255,0)\n                self.lowest_vloss=v_loss\n        acc=logs.get('accuracy')  # get training accuracy \n        v_acc=logs.get('val_accuracy')\n        loss=logs.get('loss') \n        msg=f'{str(epoch+1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{pimprov:^10.2f}{duration:^8.2f}'\n        print_in_color (msg,color, (55,65,80))\n        if self.ask:\n            if epoch + 1 ==self.ask_epoch:                \n                if self.base_model !=None and self.base_model.trainable ==  False:\n                    msg='\\n Enter H to end training,an integer for the number of additional epochs to run then ask again\\n'\n                    msg=msg + 'or T to train base model\\n'                    \n                else:\n                    msg='\\n Enter H to end training,an integer for the number of additional epochs to run then ask again\\n'\n                ans=input(msg )\n                if ans == 'T' or ans== 't':\n                    self.base_model.trainable=True\n                    msg=f'Base Model trainable is now set to {self.base_model.trainable}'\n                    print_in_color(msg, (0,255,255), (55,65,80))\n                    msg=' enter an integer for the number of additional epochs to run then ask again'\n                    ans=(input(msg))\n                    \n                if ans == 'H' or ans =='h' or ans == '0':\n                    msg=f'\\nTraining halted on epoch {epoch+1} due to user input'\n                    print_in_color(msg, (0,255,255), (55,65,80))\n                    self.model.stop_training = True\n                else:\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        msg=f'\\nYou specified maximum epochs of as {self.epochs}, cannot train for {self.ask_epoch} '\n                        print_in_color(msg, (244,252,3), (55,65,80))\n                    else:\n                        print ('\\n Training will continue to epoch ', self.ask_epoch, flush=True)\n                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:10s}{7:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', '% Improv', 'Duration')\n                        print_in_color(msg, (244,252,3), (55,65,80)) ","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:02.910540Z","iopub.execute_input":"2022-01-23T17:51:02.913149Z","iopub.status.idle":"2022-01-23T17:51:02.952717Z","shell.execute_reply.started":"2022-01-23T17:51:02.913112Z","shell.execute_reply":"2022-01-23T17:51:02.951875Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"dwell\"></a>\n# <center>DWELL Callback Definition</center>","metadata":{}},{"cell_type":"markdown","source":"The DWELL callback if useful when training a model and the validation loss on the current epoch exceeds that of the previous epoch.    \nWhen that is the case your model has moved to a point in Nspace(N being the number of trainable parameters) that is less favorable  \nthan that for the previous epoch. What the callback does is to detect this condition and if it occurs it sets the model weights to  \nthose of the  epoch with the lowesy validation loss. It also reduces the learning rate.  \nIf you do not change the learning rate on the next epoch you would end up in the same unfavorable point in Nspace.  \nUse of the callback is of the form    \ncallbacks=[DWELL(model, factor, verbose)] where :\n* model is the name of your compiled model\n* factor is a float between 0 and 1. The new learning rate is given by new_lr= current_lr * factor\n* verbose is a boolean. If verbose is True if the condition occurs, a print out is generated for the current epoch that advises the  \n  model weights have been set to the weights of the epoch with the lowest validation loss and it prints out the reduced learning rate ","metadata":{}},{"cell_type":"code","source":"class DWELL(keras.callbacks.Callback):\n    def __init__(self,model,  factor, verbose):\n        super(DWELL, self).__init__()\n        self.model=model\n        self.initial_lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it  \n        self.lowest_vloss=np.inf # set lowest validation loss to infinity initially\n        self.best_weights=self.model.get_weights() # set best weights to model's initial weights \n        self.verbose=verbose \n        self.best_epoch=0\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate         \n        vloss=logs.get('val_loss')  # get the validation loss for this epoch \n        if vloss>self.lowest_vloss:\n            self.model.set_weights(self.best_weights)\n            new_lr=lr * factor\n            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n            if self.verbose:\n                print( '\\n model weights reset to best weights from epoch ', self.best_epoch+1, ' and reduced lr to ', new_lr, flush=True)\n        else:\n            self.lowest_vloss=vloss\n            self.best_weights=self.model.get_weights()\n            self.best_epoch= epoch\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:02.957562Z","iopub.execute_input":"2022-01-23T17:51:02.958338Z","iopub.status.idle":"2022-01-23T17:51:02.972227Z","shell.execute_reply.started":"2022-01-23T17:51:02.958300Z","shell.execute_reply":"2022-01-23T17:51:02.971214Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"lra\"></a>\n# <center>LRA Callback Definition</center>","metadata":{}},{"cell_type":"markdown","source":"The LRA callback is somewhat a combination of the DWELL and SPREADSHEET callback with additional features.  \nThe form of the callback is   \ncallbacks=[LRA(model, base_model, patience,stop_patience, threshold, factor, dwell, batches,epochs, ask_epoch)] where  \n* model is the name of your compiled model\n* base_model is the name of the base_model used for transfer learning. If not doing transfer learning set base_model=None\n* patience is an integer. This callback initially monitors the training accuracy. If the training accuracy fails to improve  \n  after patience number of epochs the learning rate is reduced where new_lr=old_lr * factor.   \n  The training accuracy is monitored until it achieves or exceeds the float value threshold.   \n  At that point the callback now monitors validation loss.  \n  If the validation loss fails to decrease for patience number of epochs the learning rate is adjusted.   \n  The callback always saves the weights for the epoch with the lowest validation loss.  \n  At the conclusion of training the best weights are loaded  into the model.\n* stop_patience is an integer. If stop_patience number of consecutive epochs occur where the learning rate was reduced  \n   but the metric being monitored idi not improve the training is halted.\n* threshold is a float between 0 and 1.0. It is the level the training accuracy must meet or exceed for the callback to   \n   switch to monitoring validation loss.\n* factor is a float between 0 and 1.0. It determines the new learning rate by the formula new_lr=old_lr * factor \n* dwell is a boolean. When dwell=True the callback monitors the metric. If the metric fails to improve on the current epoch    \n   it means you have moved to a point in Nspace(N is the number of trainable parameters of the model) that is NOT as good in  \n   terms of the metric value as was the point for the epoch with the best metric performance. So the callback loads the   \n   model with the weights from that epoch. It then reduces the learning rate and continues training. If dwell=False the  \n   scenario described does not occur. However your model always ends up with the weights for the epoch with the best metric  \n   performance laded in the model.\n* batches is an integer used only for printing purposes. It's value should be the same as train_steps, that is calculated as  \n   batches = number of training samples//batch_size +1. For example if you have 1001 samples and your batch_size is 10  \n   then batches= 10 + 1 = 11.\n* epochs is an integer and is the value of epochs you use for model.fit\n* ask_epoch is an integer. Lets say it is set to a value N. After the model trains for N epochs the user is queired to provide    \n   input. If an integer is entered say M then training will continue for M more epochs and at the end of epoch M + N the    \n   user is queried again. If the user enters another integer the process continues until epochs number of epochs is run.  \n   If the user is doing transfer learning whene the vale of base_model is NOT None at the query you can enter a T. This will    \n   make the base_model trainable. After entering T you will be queried to enter an integer for how many more epochs to run  \n   before you are queried again. At any query you have the option to enter H to halt training.  \n   \n See the LRA callback demo for demonstration of use.","metadata":{}},{"cell_type":"code","source":"class LRA(keras.callbacks.Callback):\n    def __init__(self,model, base_model, patience,stop_patience, threshold, factor, dwell, batches,epochs, ask_epoch):\n        super(LRA, self).__init__()\n        self.model=model\n        self.base_model=base_model\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.stop_patience=stop_patience # specifies how many times to adjust lr without improvement to stop training\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.dwell=dwell\n        self.batches=batches # number of training batch to runn per epoch        \n        self.epochs=epochs\n        self.ask_epoch=ask_epoch\n        self.ask_epoch_initial=ask_epoch # save this value to restore if restarting training\n        # callback variables \n        self.count=0 # how many times lr has been reduced without improvement\n        self.stop_count=0        \n        self.best_epoch=1   # epoch with the lowest loss        \n        self.initial_lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it         \n        self.highest_tracc=0.0 # set highest training accuracy to 0 initially\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity initially\n        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n        self.initial_weights=self.model.get_weights()   # save initial weights if they have to get restored \n        \n    def on_train_begin(self, logs=None):        \n        if self.base_model != None:\n            status=base_model.trainable\n            if status:\n                msg=' initializing callback starting training with base_model trainable'\n            else:\n                msg='initializing callback starting training with base_model not trainable'\n        else:\n            msg='initialing callback and starting training'                        \n        print_in_color (msg, (244, 252, 3), (55,65,80)) \n        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n        print_in_color(msg, (244,252,3), (55,65,80)) \n        self.start_time= time.time()\n        \n    def on_train_end(self, logs=None):\n        stop_time=time.time()\n        tr_duration= stop_time- self.start_time            \n        hours = tr_duration // 3600\n        minutes = (tr_duration - (hours * 3600)) // 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n\n        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n        msg=f'Training is completed - model is set with weights from epoch {self.best_epoch} '\n        print_in_color(msg, (0,255,0), (55,65,80))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print_in_color(msg, (0,255,0), (55,65,80))   \n        \n    def on_train_batch_end(self, batch, logs=None):\n        acc=logs.get('accuracy')* 100  # get training accuracy \n        loss=logs.get('loss')\n        msg='{0:20s}processing batch {1:4s} of {2:5s} accuracy= {3:8.3f}  loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n        print(msg, '\\r', end='') # prints over on the same line to show running batch count        \n        \n    def on_epoch_begin(self,epoch, logs=None):\n        self.now= time.time()\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        later=time.time()\n        duration=later-self.now \n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        current_lr=lr\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        acc=logs.get('accuracy')  # get training accuracy \n        v_acc=logs.get('val_accuracy')\n        loss=logs.get('loss')        \n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            monitor='accuracy'\n            if epoch ==0:\n                pimprov=0.0\n            else:\n                pimprov= (acc-self.highest_tracc )*100/self.highest_tracc\n            if acc>self.highest_tracc: # training accuracy improved in the epoch                \n                self.highest_tracc=acc # set new highest training accuracy\n                self.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n                self.count=0 # set count to 0 since training accuracy improved\n                self.stop_count=0 # set stop counter to 0\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss\n                color= (0,255,0)\n                self.best_epoch=epoch + 1  # set the value of best epoch for this epoch              \n            else: \n                # training accuracy did not improve check if this has happened for patience number of epochs\n                # if so adjust learning rate\n                if self.count>=self.patience -1: # lr should be adjusted\n                    color=(245, 170, 66)\n                    lr= lr* self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    self.stop_count=self.stop_count + 1 # count the number of consecutive lr adjustments\n                    self.count=0 # reset counter\n                    if self.dwell:\n                        self.model.set_weights(self.best_weights) # return to better point in N space                        \n                    else:\n                        if v_loss<self.lowest_vloss:\n                            self.lowest_vloss=v_loss                                    \n                else:\n                    self.count=self.count +1 # increment patience counter                    \n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            monitor='val_loss'\n            if epoch ==0:\n                pimprov=0.0\n            else:\n                pimprov= (self.lowest_vloss- v_loss )*100/self.lowest_vloss\n            if v_loss< self.lowest_vloss: # check if the validation loss improved \n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                self.best_weights=self.model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved  \n                self.stop_count=0  \n                color=(0,255,0)                \n                self.best_epoch=epoch + 1 # set the value of the best epoch to this epoch\n            else: # validation loss did not improve\n                if self.count>=self.patience-1: # need to adjust lr\n                    color=(245, 170, 66)\n                    lr=lr * self.factor # adjust the learning rate                    \n                    self.stop_count=self.stop_count + 1 # increment stop counter because lr was adjusted \n                    self.count=0 # reset counter\n                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n                    if self.dwell:\n                        self.model.set_weights(self.best_weights) # return to better point in N space\n                else: \n                    self.count =self.count +1 # increment the patience counter                    \n                if acc>self.highest_tracc:\n                    self.highest_tracc= acc\n        msg=f'{str(epoch+1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n        print_in_color (msg,color, (55,65,80))\n        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n            print_in_color(msg, (0,255,255), (55,65,80))\n            self.model.stop_training = True # stop training\n        else: \n            if self.ask_epoch !=None:\n                if epoch + 1 >= self.ask_epoch:\n                    if base_model.trainable:\n                        msg='enter H to halt training or an integer for number of epochs to run then ask again'\n                    else:\n                        msg='enter H to halt training ,T to train the base_model, or an integer for number of epochs to run then ask again'\n                    print_in_color(msg, (0,255,255), (55,65,80))\n                    ans=input('')                    \n                    if ans=='H' or ans=='h':\n                        msg=f'training has been halted at epoch {epoch + 1} due to user input'\n                        print_in_color(msg, (0,255,255), (55,65,80))\n                        self.model.stop_training = True # stop training\n                    elif ans == 'T' or ans=='t':\n                        if base_model.trainable:\n                            msg='base_model is already set as trainable'\n                        else:\n                            msg='setting base_model as trainable for fine tuning of model'\n                            self.base_model.trainable=True\n                        print_in_color(msg, (0, 255,255), (55,65,80))\n                        msg='Enter an integer for the number of epochs to run then be asked again'\n                        print_in_color(msg, (0,2555,255), (55,65,80))\n                        ans=input()\n                        ans=int(ans)\n                        self.ask_epoch +=ans\n                        msg=f' training will continue until epoch ' + str(self.ask_epoch) \n                        print_in_color(msg, (0, 255,255), (55,65,80))\n                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n                        print_in_color(msg, (244,252,3), (55,65,80))                         \n                        self.count=0\n                        self.stop_count=0                        \n                        self.ask_epoch = epoch + 1 + self.ask_epoch_initial \n                        \n                    else:\n                        ans=int(ans)\n                        self.ask_epoch +=ans\n                        msg=f' training will continue until epoch ' + str(self.ask_epoch)                         \n                        print_in_color(msg, (0, 255,255), (55,65,80))\n                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n                        print_in_color(msg, (244,252,3), (55,65,80)) ","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:02.976681Z","iopub.execute_input":"2022-01-23T17:51:02.978811Z","iopub.status.idle":"2022-01-23T17:51:03.034245Z","shell.execute_reply.started":"2022-01-23T17:51:02.978775Z","shell.execute_reply":"2022-01-23T17:51:03.033423Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"setup\"></a>\n# <center>Setup code to demo the callbacks</center>","metadata":{}},{"cell_type":"markdown","source":"### The function below prints a text message in rgb foreground colors and rgb background colors\n### it is used by some  of the callbacks to provide a easier to see output","metadata":{}},{"cell_type":"code","source":"def print_in_color(txt_msg,fore_tupple,back_tupple,):    \n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:03.035760Z","iopub.execute_input":"2022-01-23T17:51:03.036019Z","iopub.status.idle":"2022-01-23T17:51:03.046464Z","shell.execute_reply.started":"2022-01-23T17:51:03.035963Z","shell.execute_reply":"2022-01-23T17:51:03.045505Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## input an image and get the shape","metadata":{}},{"cell_type":"code","source":"img_path=r'../input/chess-pieces-detection-images-dataset/KnightImages/00000001.jpg'\nimg=plt.imread(img_path)\nprint ('Image shape is: ', img.shape)\nplt.axis('off')\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:03.051277Z","iopub.execute_input":"2022-01-23T17:51:03.052083Z","iopub.status.idle":"2022-01-23T17:51:03.317791Z","shell.execute_reply.started":"2022-01-23T17:51:03.052044Z","shell.execute_reply":"2022-01-23T17:51:03.317119Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Image shape is:  (1024, 1024, 3)\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f61ade87f50>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"### create a dataframe of the form filepaths(path to the image file), labels (class label of the image file)","metadata":{}},{"cell_type":"code","source":"sdir=r'../input/chess-pieces-detection-images-dataset'\nclasslist=os.listdir(sdir)\nlabels=[]\nfilepaths=[]\nfor klass in classlist:\n    classpath=os.path.join(sdir, klass)\n    flist=os.listdir(classpath)\n    for f in flist:\n        fpath=os.path.join(classpath,f)\n        filepaths.append(fpath)\n        labels.append(klass)\nFseries=pd.Series(filepaths, name='filepaths')\nLseries = pd.Series(labels, name='labels')\ndf=pd.concat([Fseries, Lseries], axis=1)\nclass_count=len(list(df['labels'].unique()))\nprint('Number of classes in dataset is ', class_count)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:03.318888Z","iopub.execute_input":"2022-01-23T17:51:03.320316Z","iopub.status.idle":"2022-01-23T17:51:03.339097Z","shell.execute_reply.started":"2022-01-23T17:51:03.320274Z","shell.execute_reply":"2022-01-23T17:51:03.338372Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Number of classes in dataset is  5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# split df into a train_df, a test_df and a valid df","metadata":{}},{"cell_type":"code","source":"train_df, dummy_df =train_test_split(df, train_size=.9, shuffle=True, random_state=123, stratify= df['labels'])\nvalid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify =dummy_df['labels'])\nprint('train_df length: ', len(train_df), '  test_df length: ',len(test_df), '  valid_df length: ', len(valid_df))\nprint (train_df['labels'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:03.340355Z","iopub.execute_input":"2022-01-23T17:51:03.340589Z","iopub.status.idle":"2022-01-23T17:51:03.355549Z","shell.execute_reply.started":"2022-01-23T17:51:03.340555Z","shell.execute_reply":"2022-01-23T17:51:03.354706Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"train_df length:  606   test_df length:  34   valid_df length:  34\nKnightImages    156\nbishopImages    148\nRookImage       125\nQueenimages     103\nPawnImages       74\nName: labels, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## train_df is  not balanced the trim function below will limit the number of samples in each class to max_samples which\n## will be set to 74. This will of course reduce accuracy but I want to have short training times. I also limit the\n## image size to 100 X 100 to speed up training","metadata":{}},{"cell_type":"code","source":"def trim (df, max_size, min_size, column):\n    df=df.copy()\n    original_class_count= len(list(df[column].unique()))\n    print ('Original Number of classes in dataframe: ', original_class_count)\n    sample_list=[] \n    groups=df.groupby(column)\n    for label in df[column].unique():        \n        group=groups.get_group(label)\n        sample_count=len(group)         \n        if sample_count> max_size :\n            strat=group[column]\n            samples,_=train_test_split(group, train_size=max_size, shuffle=True, random_state=123, stratify=strat)            \n            sample_list.append(samples)\n        elif sample_count>= min_size:\n            sample_list.append(group)\n    df=pd.concat(sample_list, axis=0).reset_index(drop=True)\n    final_class_count= len(list(df[column].unique())) \n    if final_class_count != original_class_count:\n        print ('*** WARNING***  dataframe has a reduced number of classes' )\n    balance=list(df[column].value_counts())\n    print (balance)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:03.357060Z","iopub.execute_input":"2022-01-23T17:51:03.357398Z","iopub.status.idle":"2022-01-23T17:51:03.365740Z","shell.execute_reply.started":"2022-01-23T17:51:03.357361Z","shell.execute_reply":"2022-01-23T17:51:03.364820Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"max_samples=74\nmin_samples=0\ncolumn='labels'\ntrain_df=trim(train_df, max_samples, min_samples, column, )","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:03.367350Z","iopub.execute_input":"2022-01-23T17:51:03.367926Z","iopub.status.idle":"2022-01-23T17:51:03.388209Z","shell.execute_reply.started":"2022-01-23T17:51:03.367859Z","shell.execute_reply":"2022-01-23T17:51:03.387393Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Original Number of classes in dataframe:  5\n[74, 74, 74, 74, 74]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## all classes now have 74 training samples. Create train, test and validation generators","metadata":{}},{"cell_type":"code","source":"img_size = (100,100)\nworking_dir = r'./'\nbatch_size= 40\n# calculate test_batch_size and test_step so we go through test files exactly once\nlength=len(test_df)\ntest_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \ntest_steps=int(length/test_batch_size)\nprint ('test batch size= ', test_batch_size, '  test steps= ', test_steps)\ntrgen=ImageDataGenerator(horizontal_flip=True)\ntvgen=ImageDataGenerator()\ntrain_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\nvalid_gen=tvgen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\ntest_gen=tvgen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:03.389798Z","iopub.execute_input":"2022-01-23T17:51:03.390095Z","iopub.status.idle":"2022-01-23T17:51:03.427552Z","shell.execute_reply.started":"2022-01-23T17:51:03.390056Z","shell.execute_reply":"2022-01-23T17:51:03.426801Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"test batch size=  34   test steps=  1\nFound 370 validated image filenames belonging to 5 classes.\nFound 34 validated image filenames belonging to 5 classes.\nFound 34 validated image filenames belonging to 5 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Use transfer learning with EfficientNetB3 model\nThis function creates the model. It is set up for transfer learning. Parameter trainable determines\nif the base model is trainable. The TLC callback can be used to control this state of the base model\nfor those that want to start training with the base model not trainable then after a specified number\nof epochs make the base_model trainable","metadata":{}},{"cell_type":"code","source":"def make_model(img_img_size, class_count,lr=.001, trainable=True):\n    img_shape=(img_size[0], img_size[1], 3)\n    model_name='EfficientNetB3'\n    base_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n    base_model.trainable=trainable\n    x=base_model.output\n    x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n    x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                    bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n    x=Dropout(rate=.45, seed=123)(x)        \n    output=Dense(class_count, activation='softmax')(x)\n    model=Model(inputs=base_model.input, outputs=output)\n    model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) \n    return model, base_model # return the base_model so the callback can control its training state","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:03.428963Z","iopub.execute_input":"2022-01-23T17:51:03.429229Z","iopub.status.idle":"2022-01-23T17:51:03.438076Z","shell.execute_reply.started":"2022-01-23T17:51:03.429193Z","shell.execute_reply":"2022-01-23T17:51:03.437075Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"askdemo\"></a>\n# <center>ASK Callback Demonstration</center>","metadata":{}},{"cell_type":"code","source":"epochs=40 # max number of epochs to run unless training is halted by the user\nask_epoch=5 # set number of epochs to run then be quired to contiue or halt\nlr=.002 # set the optimizer learning rate\ntrainable=True # set base_model trainable\nmodel, _=make_model(img_size, class_count, lr,)\ncallbacks=[ASK(model, epochs,  ask_epoch)]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:51:03.439621Z","iopub.execute_input":"2022-01-23T17:51:03.440207Z","iopub.status.idle":"2022-01-23T17:51:06.998775Z","shell.execute_reply.started":"2022-01-23T17:51:03.440170Z","shell.execute_reply":"2022-01-23T17:51:06.998043Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## train the model ","metadata":{}},{"cell_type":"code","source":"history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-23T17:51:06.999872Z","iopub.execute_input":"2022-01-23T17:51:07.000109Z","iopub.status.idle":"2022-01-23T17:52:11.474753Z","shell.execute_reply.started":"2022-01-23T17:51:07.000076Z","shell.execute_reply":"2022-01-23T17:52:11.474032Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Training will proceed until epoch 5  then you will be asked to\n enter H to halt training or enter an integer for how many more epochs to run then be asked again\nEpoch 1/40\n 6/10 [=================>............] - ETA: 1s - loss: 10.3243 - accuracy: 0.2417","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n","output_type":"stream"},{"name":"stdout","text":"10/10 [==============================] - 19s 762ms/step - loss: 10.0993 - accuracy: 0.3081 - val_loss: 13.8539 - val_accuracy: 0.2647\nEpoch 2/40\n10/10 [==============================] - 5s 501ms/step - loss: 8.8755 - accuracy: 0.5270 - val_loss: 10.9114 - val_accuracy: 0.4706\nEpoch 3/40\n10/10 [==============================] - 5s 496ms/step - loss: 8.0016 - accuracy: 0.6919 - val_loss: 11.6428 - val_accuracy: 0.5294\nEpoch 4/40\n10/10 [==============================] - 5s 424ms/step - loss: 7.4355 - accuracy: 0.7649 - val_loss: 9.5179 - val_accuracy: 0.5882\nEpoch 5/40\n10/10 [==============================] - 5s 466ms/step - loss: 6.9288 - accuracy: 0.8514 - val_loss: 8.3931 - val_accuracy: 0.6765\n\n Enter H to end training or  an integer for the number of additional epochs to run then ask again\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 3\n"},{"name":"stdout","text":"you entered  3  Training will continue to epoch  8\nEpoch 6/40\n10/10 [==============================] - 5s 489ms/step - loss: 6.5038 - accuracy: 0.9081 - val_loss: 7.8253 - val_accuracy: 0.6765\nEpoch 7/40\n10/10 [==============================] - 5s 492ms/step - loss: 6.2238 - accuracy: 0.8649 - val_loss: 6.9767 - val_accuracy: 0.7353\nEpoch 8/40\n10/10 [==============================] - 5s 487ms/step - loss: 5.9284 - accuracy: 0.9054 - val_loss: 6.3513 - val_accuracy: 0.8235\n\n Enter H to end training or  an integer for the number of additional epochs to run then ask again\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" h\n"},{"name":"stdout","text":"you entered  h  Training halted on epoch  8  due to user input\n\ntraining elapsed time was 0.0 hours,  1.0 minutes, 4.01 seconds)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"slrdemo\"></a>\n# <center>SLR Callback Demonstration</center>","metadata":{}},{"cell_type":"code","source":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\nmodel, _=make_model(img_size, class_count) # recreate the model - so it starts fresh with new initialized weights\nask_epoch=5 # epoch to query user\nepochs=40  # total epochs to run unless halted by the callback\ncallbacks=[SLR(model, epochs,  ask_epoch)] # instantiate the callback\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:52:11.475887Z","iopub.execute_input":"2022-01-23T17:52:11.476318Z","iopub.status.idle":"2022-01-23T17:54:34.606799Z","shell.execute_reply.started":"2022-01-23T17:52:11.476276Z","shell.execute_reply":"2022-01-23T17:54:34.606046Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Training will proceed until epoch 5  then you will be asked to\n enter H to halt training, or enter an integer for how many more epochs to run, then be asked again\n or enter an A to adjust the learning rate. If an A is entered you will be queired to enter a float\n values for the new learning rate then be asked to enter an integer for howmany more epochs to run before being asked again\nEpoch 1/40\n10/10 [==============================] - 20s 767ms/step - loss: 9.8134 - accuracy: 0.3000 - val_loss: 10.7592 - val_accuracy: 0.3529\nEpoch 2/40\n10/10 [==============================] - 5s 495ms/step - loss: 8.8045 - accuracy: 0.5622 - val_loss: 9.1871 - val_accuracy: 0.4706\nEpoch 3/40\n10/10 [==============================] - 5s 498ms/step - loss: 8.1247 - accuracy: 0.7378 - val_loss: 8.9415 - val_accuracy: 0.4412\nEpoch 4/40\n10/10 [==============================] - 5s 494ms/step - loss: 7.7688 - accuracy: 0.8081 - val_loss: 8.9533 - val_accuracy: 0.5294\nEpoch 5/40\n10/10 [==============================] - 6s 560ms/step - loss: 7.4852 - accuracy: 0.8405 - val_loss: 8.6377 - val_accuracy: 0.5588\n\n Enter H to end training or an integer for the number of additional epochs to run or enter A to adjust the learning rate\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 2\n"},{"name":"stdout","text":"you entered  2  Training will continue to epoch  7\nEpoch 6/40\n10/10 [==============================] - 5s 497ms/step - loss: 7.2039 - accuracy: 0.9054 - val_loss: 8.1911 - val_accuracy: 0.7059\nEpoch 7/40\n10/10 [==============================] - 5s 452ms/step - loss: 7.0336 - accuracy: 0.9216 - val_loss: 7.6929 - val_accuracy: 0.7353\n\n Enter H to end training or an integer for the number of additional epochs to run or enter A to adjust the learning rate\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" A\n"},{"name":"stdout","text":" current lr =  0.0010000000474974513 Enter a float value for the new learning rate\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" .0005\n"},{"name":"stdout","text":"Enter an integer for the number of additional epochs to run then be asked again\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 4\n"},{"name":"stdout","text":"you entered  4  Training will continue to epoch  11\nEpoch 8/40\n10/10 [==============================] - 5s 446ms/step - loss: 6.8831 - accuracy: 0.9405 - val_loss: 7.4516 - val_accuracy: 0.7059\nEpoch 9/40\n10/10 [==============================] - 5s 411ms/step - loss: 6.8544 - accuracy: 0.9216 - val_loss: 7.2875 - val_accuracy: 0.7059\nEpoch 10/40\n10/10 [==============================] - 5s 475ms/step - loss: 6.7145 - accuracy: 0.9486 - val_loss: 7.1347 - val_accuracy: 0.7353\nEpoch 11/40\n10/10 [==============================] - 5s 501ms/step - loss: 6.6184 - accuracy: 0.9568 - val_loss: 7.0453 - val_accuracy: 0.7059\n\n Enter H to end training or an integer for the number of additional epochs to run or enter A to adjust the learning rate\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" a\n"},{"name":"stdout","text":" current lr =  0.0005000000237487257 Enter a float value for the new learning rate\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" .001\n"},{"name":"stdout","text":"Enter an integer for the number of additional epochs to run then be asked again\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 2\n"},{"name":"stdout","text":"you entered  2  Training will continue to epoch  13\nEpoch 12/40\n10/10 [==============================] - 5s 488ms/step - loss: 6.5208 - accuracy: 0.9568 - val_loss: 6.9046 - val_accuracy: 0.7059\nEpoch 13/40\n10/10 [==============================] - 5s 486ms/step - loss: 6.3961 - accuracy: 0.9568 - val_loss: 6.7218 - val_accuracy: 0.7647\n\n Enter H to end training or an integer for the number of additional epochs to run or enter A to adjust the learning rate\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" h\n"},{"name":"stdout","text":"you entered  h  Training halted on epoch  13  due to user input\n\ntraining elapsed time was 0.0 hours,  2.0 minutes, 20.18 seconds)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"somtdemo\"></a>\n# <center>SOMT Callback Demonstration</center>","metadata":{}},{"cell_type":"code","source":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\nmodel, _=make_model(img_size, class_count) # recreate the model - so it starts fresh with new initialized weights\nepochs=40 #  total epochs to run unless halted by the callback\ntr_thold=.8 # set training accuracy threshold\nv_thold=.75 # set validation accuracy threshold\ncallbacks=[SOMT(model, tr_thold, v_thold)] # instantiate the callback\n# ****************NOTE - set verbose=0 in model.fit so printouts do not conflict**********************\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:54:34.608045Z","iopub.execute_input":"2022-01-23T17:54:34.608478Z","iopub.status.idle":"2022-01-23T17:56:49.255659Z","shell.execute_reply.started":"2022-01-23T17:54:34.608437Z","shell.execute_reply":"2022-01-23T17:56:49.254835Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Starting Training - training will halt if training accuracy achieves or exceeds  0.8\nand validation accuracy meets or exceeds  0.75\n Epoch   Train Acc   Train Loss  Valid Acc   Valid_Loss   Duration  \n   1         0.30       9.7579       0.29      11.8249      20.23    \n   2         0.57       8.7865       0.50       9.4722       4.87    \n   3         0.72       8.1335       0.56       9.2530       5.58    \n   4         0.79       7.7188       0.56       9.0198       4.82    \n   5         0.86       7.3901       0.71       8.3449       5.12    \n   6         0.88       7.1788       0.71       7.7619       4.87    \n   7         0.92       6.9319       0.71       7.5456       5.07    \n   8         0.90       6.8250       0.68       7.4113       4.78    \n   9         0.93       6.6043       0.71       7.1365       5.53    \n   10        0.95       6.4410       0.71       6.9906       4.81    \n   11        0.94       6.2689       0.71       6.8380       4.76    \n   12        0.94       6.1406       0.68       6.7355       5.09    \n   13        0.96       5.9739       0.65       6.6054       4.85    \n   14        0.96       5.8402       0.68       6.4871       5.35    \n   15        0.96       5.7265       0.68       6.4131       4.84    \n   16        0.95       5.5995       0.71       6.3308       5.44    \n   17        0.96       5.4632       0.71       6.2401       4.80    \n   18        0.95       5.3589       0.71       6.1093       5.03    \n   19        0.97       5.2260       0.71       5.9654       4.87    \n   20        0.98       5.0993       0.74       5.8578       5.09    \n   21        0.97       5.0107       0.74       5.7301       4.86    \n   22        0.97       4.9179       0.74       5.5717       5.65    \n   23        0.97       4.8007       0.76       5.4347       4.87    \n\ntraining accuracy and validation accuracy reached the thresholds on epoch 23\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"tlcdemo\"></a>\n# <center>TLC Callback Demonstration</center>","metadata":{}},{"cell_type":"code","source":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\n# ************NOTE base_model trainable set equal to False in code below when model is created*************\nmodel, base_model=make_model(img_size, class_count, trainable=False) # recreate the model - so it starts fresh with new initialized weights\nepochs=40 #  total epochs to run unless halted by the callback\nask_epoch=5\ncallbacks=[TLC(model, base_model, epochs,  ask_epoch)] # instantiate the callback\n\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:56:49.257377Z","iopub.execute_input":"2022-01-23T17:56:49.257625Z","iopub.status.idle":"2022-01-23T17:59:23.734500Z","shell.execute_reply.started":"2022-01-23T17:56:49.257591Z","shell.execute_reply":"2022-01-23T17:59:23.733703Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Training will proceed until epoch 5, then you will be asked to either halt, continue \nor make the base model trainable then asked to enter number of epochs to run then ask again\n\nEpoch 1/40\n10/10 [==============================] - 14s 709ms/step - loss: 9.7980 - accuracy: 0.3243 - val_loss: 9.5893 - val_accuracy: 0.4706\nEpoch 2/40\n10/10 [==============================] - 4s 443ms/step - loss: 8.7069 - accuracy: 0.5405 - val_loss: 8.9553 - val_accuracy: 0.6176\nEpoch 3/40\n10/10 [==============================] - 4s 445ms/step - loss: 8.3771 - accuracy: 0.6486 - val_loss: 8.6678 - val_accuracy: 0.5882\nEpoch 4/40\n10/10 [==============================] - 5s 536ms/step - loss: 7.9827 - accuracy: 0.6919 - val_loss: 8.3664 - val_accuracy: 0.5588\nEpoch 5/40\n10/10 [==============================] - 4s 454ms/step - loss: 7.6550 - accuracy: 0.7405 - val_loss: 7.9790 - val_accuracy: 0.5882\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n Enter H to end training,an integer for the number of additional epochs to run then ask again\nor T to train base model\n 5\n"},{"name":"stdout","text":"\n Training will continue to epoch  10\nEpoch 6/40\n10/10 [==============================] - 4s 447ms/step - loss: 7.4310 - accuracy: 0.7622 - val_loss: 7.7937 - val_accuracy: 0.7059\nEpoch 7/40\n10/10 [==============================] - 5s 472ms/step - loss: 7.3078 - accuracy: 0.7622 - val_loss: 7.6479 - val_accuracy: 0.6765\nEpoch 8/40\n10/10 [==============================] - 4s 443ms/step - loss: 7.0443 - accuracy: 0.8000 - val_loss: 7.4941 - val_accuracy: 0.6471\nEpoch 9/40\n10/10 [==============================] - 5s 540ms/step - loss: 6.9201 - accuracy: 0.8135 - val_loss: 7.2742 - val_accuracy: 0.6471\nEpoch 10/40\n10/10 [==============================] - 4s 378ms/step - loss: 6.6795 - accuracy: 0.8568 - val_loss: 7.0732 - val_accuracy: 0.6471\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n Enter H to end training,an integer for the number of additional epochs to run then ask again\nor T to train base model\n T\n"},{"name":"stdout","text":"Base Model trainable is now set to  True\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n enter an integer for the number of additional epochs to run then ask again\n 5\n"},{"name":"stdout","text":"\n Training will continue to epoch  15\nEpoch 11/40\n10/10 [==============================] - 5s 426ms/step - loss: 6.5994 - accuracy: 0.8351 - val_loss: 6.9247 - val_accuracy: 0.6471\nEpoch 12/40\n10/10 [==============================] - 4s 447ms/step - loss: 6.3866 - accuracy: 0.8811 - val_loss: 6.8242 - val_accuracy: 0.6471\nEpoch 13/40\n10/10 [==============================] - 4s 450ms/step - loss: 6.2584 - accuracy: 0.8811 - val_loss: 6.6970 - val_accuracy: 0.6176\nEpoch 14/40\n10/10 [==============================] - 5s 384ms/step - loss: 6.1337 - accuracy: 0.9000 - val_loss: 6.5565 - val_accuracy: 0.6471\nEpoch 15/40\n10/10 [==============================] - 4s 443ms/step - loss: 6.0031 - accuracy: 0.9081 - val_loss: 6.4736 - val_accuracy: 0.5882\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n Enter H to end training,an integer for the number of additional epochs to run then ask again\n 5\n"},{"name":"stdout","text":"\n Training will continue to epoch  20\nEpoch 16/40\n10/10 [==============================] - 4s 444ms/step - loss: 5.8579 - accuracy: 0.9216 - val_loss: 6.3790 - val_accuracy: 0.5882\nEpoch 17/40\n10/10 [==============================] - 4s 453ms/step - loss: 5.7631 - accuracy: 0.8811 - val_loss: 6.2282 - val_accuracy: 0.6176\nEpoch 18/40\n10/10 [==============================] - 5s 482ms/step - loss: 5.6381 - accuracy: 0.9000 - val_loss: 6.1155 - val_accuracy: 0.6176\nEpoch 19/40\n10/10 [==============================] - 4s 445ms/step - loss: 5.5157 - accuracy: 0.9135 - val_loss: 6.0262 - val_accuracy: 0.5882\nEpoch 20/40\n10/10 [==============================] - 5s 489ms/step - loss: 5.4730 - accuracy: 0.8946 - val_loss: 5.9384 - val_accuracy: 0.5882\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n Enter H to end training,an integer for the number of additional epochs to run then ask again\n h\n"},{"name":"stdout","text":"\nTraining halted on epoch  20  due to user input\n\ntraining elapsed time was 0.0 hours,  2.0 minutes, 31.71 seconds)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"spreadsheetdemo\"></a>\n# <center>SPREADSHEET Callback Demonstration</center>","metadata":{}},{"cell_type":"code","source":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\nmodel, base_model=make_model(img_size, class_count, trainable=False) # recreate the model - so it starts fresh with new initialized weights\nepochs=40 #  total epochs to run unless halted by the callback\nask_epoch=5\nbatches=int(np.ceil(len(train_gen.labels)/batch_size)) # number of batches in an epoch is number of samples/batch_size used in generator\n# *****NOTE the SPREADSHEET callback requires the print_in_color function be present\ncallbacks=[SPREADSHEET(model, base_model, epochs,  ask_epoch, batches)] # instantiate the callback\n#************NOTE set verbose=0 to prevent model.fit printout*******************\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T17:59:23.735849Z","iopub.execute_input":"2022-01-23T17:59:23.736113Z","iopub.status.idle":"2022-01-23T18:01:14.523906Z","shell.execute_reply.started":"2022-01-23T17:59:23.736077Z","shell.execute_reply":"2022-01-23T18:01:14.523224Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"\u001b[38;2;0;255;255;48;2;55;65;80mTraining will proceed until epoch 5, then you will be asked to either halt, continue \nor make the base model trainable then asked to enter number of epochs to run then ask again\n\n\u001b[0m\n\u001b[38;2;244;252;3;48;2;55;65;80m Epoch     Loss   Accuracy  V_loss    V_acc     LR    % Improv  Duration\n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 1 /40     9.775   29.189   9.44262  47.059   0.00100    0.00    14.40  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 2 /40     8.848   58.378   8.75210  67.647   0.00100    7.31     4.40  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 3 /40     8.301   64.865   8.62869  58.824   0.00100    1.41     4.83  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 4 /40     8.056   68.108   8.52091  55.882   0.00100    1.25     4.42  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 5 /40     7.745   75.135   8.20717  55.882   0.00100    3.68     4.91  \n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n Enter H to end training,an integer for the number of additional epochs to run then ask again\nor T to train base model\n 5\n"},{"name":"stdout","text":"\n Training will continue to epoch  10\n\u001b[38;2;244;252;3;48;2;55;65;80m Epoch     Loss   Accuracy  V_loss    V_acc     LR    % Improv  Duration\n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 6 /40     7.558   75.946   7.92630  58.824   0.00100    3.42     4.81  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 7 /40     7.362   79.730   7.69108  55.882   0.00100    2.97     4.70  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 8 /40     7.124   81.622   7.46893  58.824   0.00100    2.89     4.60  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 9 /40     7.001   82.703   7.31548  61.765   0.00100    2.05     4.47  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m10 /40     6.806   85.135   7.21430  61.765   0.00100    1.38     4.98  \n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n Enter H to end training,an integer for the number of additional epochs to run then ask again\nor T to train base model\n T\n"},{"name":"stdout","text":"\u001b[38;2;0;255;255;48;2;55;65;80mBase Model trainable is now set to True\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" enter an integer for the number of additional epochs to run then ask again 5\n"},{"name":"stdout","text":"\n Training will continue to epoch  15\n\u001b[38;2;244;252;3;48;2;55;65;80m Epoch     Loss   Accuracy  V_loss    V_acc     LR    % Improv  Duration\n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m11 /40     6.655   87.568   7.08933  64.706   0.00100    1.73     4.71  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m12 /40     6.522   86.216   6.91227  64.706   0.00100    2.50     4.47  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m13 /40     6.409   86.757   6.79523  61.765   0.00100    1.69     4.64  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m14 /40     6.272   84.865   6.70533  61.765   0.00100    1.32     4.50  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m15 /40     6.087   91.622   6.60357  64.706   0.00100    1.52     4.78  \n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n Enter H to end training,an integer for the number of additional epochs to run then ask again\n h\n"},{"name":"stdout","text":"\u001b[38;2;0;255;255;48;2;55;65;80m\nTraining halted on epoch 15 due to user input\n\u001b[0m\n\u001b[38;2;0;255;255;48;2;55;65;80mtraining elapsed time was 0.0 hours,  1.0 minutes, 46.31 seconds)\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"dwelldemo\"></a>\n# <center>DWELL Callback Demonstration</center>","metadata":{}},{"cell_type":"code","source":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\n# **********NOTE set an initial large learning rate so as to induce need to reduce it as epochs increase\nmodel, base_model=make_model(img_size, class_count, lr=.05, trainable=True) # recreate the model - so it starts fresh with new initialized weights\nepochs=30 #  total epochs to run unless halted by the callback\nfactor= .5 # if validation loss this epoch>previous epoch new_lr= lr * factor\nverbose=1 # print out when learning rate is adjusted\ncallbacks=[DWELL(model, factor, verbose)] # instantiate the callback\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T18:01:14.525101Z","iopub.execute_input":"2022-01-23T18:01:14.525635Z","iopub.status.idle":"2022-01-23T18:04:09.296999Z","shell.execute_reply.started":"2022-01-23T18:01:14.525593Z","shell.execute_reply":"2022-01-23T18:04:09.296209Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Epoch 1/30\n10/10 [==============================] - 21s 813ms/step - loss: 66.0497 - accuracy: 0.2000 - val_loss: 23638235165353950254869774336.0000 - val_accuracy: 0.2647\nEpoch 2/30\n10/10 [==============================] - 5s 515ms/step - loss: 68.5122 - accuracy: 0.2000 - val_loss: 31376125992127694897152.0000 - val_accuracy: 0.1765\nEpoch 3/30\n10/10 [==============================] - 5s 494ms/step - loss: 54.5354 - accuracy: 0.2270 - val_loss: 259595623030849536.0000 - val_accuracy: 0.2647\nEpoch 4/30\n10/10 [==============================] - 5s 521ms/step - loss: 29.7598 - accuracy: 0.2270 - val_loss: 206831330787328.0000 - val_accuracy: 0.2059\nEpoch 5/30\n10/10 [==============================] - 5s 423ms/step - loss: 18.6920 - accuracy: 0.1892 - val_loss: 6521013403648.0000 - val_accuracy: 0.2059\nEpoch 6/30\n10/10 [==============================] - 5s 521ms/step - loss: 12.7221 - accuracy: 0.2351 - val_loss: 62812397568.0000 - val_accuracy: 0.2059\nEpoch 7/30\n10/10 [==============================] - 5s 548ms/step - loss: 9.8507 - accuracy: 0.2324 - val_loss: 882321216.0000 - val_accuracy: 0.2059\nEpoch 8/30\n10/10 [==============================] - 5s 503ms/step - loss: 7.6775 - accuracy: 0.2162 - val_loss: 16500392.0000 - val_accuracy: 0.2059\nEpoch 9/30\n10/10 [==============================] - 5s 463ms/step - loss: 4.9292 - accuracy: 0.2432 - val_loss: 609068.5625 - val_accuracy: 0.2059\nEpoch 10/30\n10/10 [==============================] - 5s 491ms/step - loss: 3.7953 - accuracy: 0.1892 - val_loss: 85345.6484 - val_accuracy: 0.2059\nEpoch 11/30\n10/10 [==============================] - 5s 483ms/step - loss: 3.1045 - accuracy: 0.1730 - val_loss: 17539.2559 - val_accuracy: 0.2059\nEpoch 12/30\n10/10 [==============================] - 5s 459ms/step - loss: 2.8104 - accuracy: 0.2189 - val_loss: 4046.4172 - val_accuracy: 0.2059\nEpoch 13/30\n10/10 [==============================] - 5s 495ms/step - loss: 2.3692 - accuracy: 0.1892 - val_loss: 1114.5686 - val_accuracy: 0.2059\nEpoch 14/30\n10/10 [==============================] - 5s 398ms/step - loss: 2.1672 - accuracy: 0.1730 - val_loss: 366.5749 - val_accuracy: 0.2059\nEpoch 15/30\n10/10 [==============================] - 5s 512ms/step - loss: 2.0060 - accuracy: 0.1919 - val_loss: 115.0423 - val_accuracy: 0.1765\nEpoch 16/30\n10/10 [==============================] - 5s 500ms/step - loss: 1.9127 - accuracy: 0.1946 - val_loss: 60.0067 - val_accuracy: 0.2353\nEpoch 17/30\n10/10 [==============================] - 5s 545ms/step - loss: 1.8712 - accuracy: 0.1838 - val_loss: 30.8034 - val_accuracy: 0.2647\nEpoch 18/30\n10/10 [==============================] - 5s 492ms/step - loss: 1.9675 - accuracy: 0.2189 - val_loss: 23.1213 - val_accuracy: 0.2059\nEpoch 19/30\n10/10 [==============================] - 5s 474ms/step - loss: 2.1358 - accuracy: 0.1649 - val_loss: 13.5409 - val_accuracy: 0.2647\nEpoch 20/30\n10/10 [==============================] - 5s 529ms/step - loss: 2.0218 - accuracy: 0.1946 - val_loss: 9.3784 - val_accuracy: 0.2647\nEpoch 21/30\n10/10 [==============================] - 5s 520ms/step - loss: 2.1431 - accuracy: 0.1892 - val_loss: 5.6890 - val_accuracy: 0.2353\nEpoch 22/30\n10/10 [==============================] - 5s 411ms/step - loss: 1.9029 - accuracy: 0.2054 - val_loss: 3.6970 - val_accuracy: 0.1176\nEpoch 23/30\n10/10 [==============================] - 5s 510ms/step - loss: 1.8729 - accuracy: 0.2054 - val_loss: 2.8135 - val_accuracy: 0.1471\nEpoch 24/30\n10/10 [==============================] - 5s 414ms/step - loss: 1.8824 - accuracy: 0.2054 - val_loss: 2.0046 - val_accuracy: 0.1471\nEpoch 25/30\n10/10 [==============================] - 5s 506ms/step - loss: 1.8444 - accuracy: 0.1838 - val_loss: 1.8821 - val_accuracy: 0.2353\nEpoch 26/30\n10/10 [==============================] - 5s 470ms/step - loss: 1.9681 - accuracy: 0.2000 - val_loss: 1.8878 - val_accuracy: 0.2353\n\n model weights reset to best weights from epoch  25  and reduced lr to  0.02500000037252903\nEpoch 27/30\n10/10 [==============================] - 5s 493ms/step - loss: 1.8171 - accuracy: 0.1892 - val_loss: 1.8094 - val_accuracy: 0.2059\nEpoch 28/30\n10/10 [==============================] - 5s 496ms/step - loss: 1.7599 - accuracy: 0.2027 - val_loss: 1.7461 - val_accuracy: 0.2059\nEpoch 29/30\n10/10 [==============================] - 5s 519ms/step - loss: 1.7138 - accuracy: 0.1973 - val_loss: 1.7141 - val_accuracy: 0.1765\nEpoch 30/30\n10/10 [==============================] - 5s 495ms/step - loss: 1.6888 - accuracy: 0.2000 - val_loss: 1.6827 - val_accuracy: 0.1765\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"lrademo\"></a>\n# <center>LRA Callback Demonstration</center>","metadata":{}},{"cell_type":"code","source":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\nmodel, base_model=make_model(img_size, class_count, trainable=False) # recreate the model - so it starts fresh with new initialized weights\nepochs=40 #  total epochs to run unless halted by the callback\nask_epoch=5 # at the end of the 5th epoch user is queried for input.\nbatches=np.ceil( len(train_gen)/batch_size) # number of batches in an epoch is number of samples/batch_size used in generator\n# *****NOTE the LRA callback requires the print_in_color function be present\npatience=1 # if monitored metric ddoes not improve for patience epochs then adjust the learning rate\nstop_patience=4 # halt training if the lr has been adjust stop patience number of times with no improvement in metric being monitored\ntr_thold=.9 # Callback initially monitor training accuracy. If it exceeds the value of tr_thold the callback switches to monitor validation loss\ndwell=True # If lr is adjusted and dwell=True model weights are set to the weights from the epoch with the best metric performance thus far\ncallbacks=[LRA(model, base_model, patience,stop_patience, tr_thold, factor, dwell, batches, epochs, ask_epoch)] # instantiate the callback\n# Note we are doing transfer learning with the base_model initially set as not trainable\n#************NOTE set verbose=0 to prevent model.fit printout*******************\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T18:08:45.664997Z","iopub.execute_input":"2022-01-23T18:08:45.665836Z","iopub.status.idle":"2022-01-23T18:10:40.928431Z","shell.execute_reply.started":"2022-01-23T18:08:45.665790Z","shell.execute_reply":"2022-01-23T18:10:40.927690Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"\u001b[38;2;244;252;3;48;2;55;65;80minitializing callback starting training with base_model not trainable\n\u001b[0m\n\u001b[38;2;244;252;3;48;2;55;65;80m Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration\n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 1 /40     9.688   32.703   9.24717  55.882   0.00100  0.00100  accuracy     0.00    14.23  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 2 /40     8.901   52.162   9.35658  50.000   0.00100  0.00100  accuracy    59.50     4.38  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 3 /40     8.389   62.162   8.63210  52.941   0.00100  0.00100  accuracy    19.17     4.67  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 4 /40     8.096   68.378   8.30225  58.824   0.00100  0.00100  accuracy    10.00     4.89  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 5 /40     7.705   75.405   8.19931  50.000   0.00100  0.00100  accuracy    10.28     4.62  \n\u001b[0m\n\u001b[38;2;0;255;255;48;2;55;65;80menter H to halt training ,T to train the base_model, or an integer for number of epochs to run then ask again\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" t\n"},{"name":"stdout","text":"\u001b[38;2;0;255;255;48;2;55;65;80msetting base_model as trainable for fine tuning of model\n\u001b[0m\n\u001b[38;2;0;2555;255;48;2;55;65;80mEnter an integer for the number of epochs to run then be asked again\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 5\n"},{"name":"stdout","text":"\u001b[38;2;0;255;255;48;2;55;65;80m training will continue until epoch 10\n\u001b[0m\n\u001b[38;2;244;252;3;48;2;55;65;80m Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv\n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 6 /40     7.537   75.676   7.94711  50.000   0.00100  0.00100  accuracy     0.36     4.35  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 7 /40     7.304   80.270   7.75384  52.941   0.00100  0.00100  accuracy     6.07     4.39  \n\u001b[0m\n\u001b[38;2;245;170;66;48;2;55;65;80m 8 /40     7.173   79.189   7.60342  55.882   0.00100  0.00050  accuracy    -1.35     4.82  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m 9 /40     7.157   81.081   7.65286  55.882   0.00050  0.00050  accuracy     1.01     4.97  \n\u001b[0m\n\u001b[38;2;245;170;66;48;2;55;65;80m10 /40     7.097   80.541   7.54498  55.882   0.00050  0.00025  accuracy    -0.67     4.65  \n\u001b[0m\n\u001b[38;2;0;255;255;48;2;55;65;80menter H to halt training or an integer for number of epochs to run then ask again\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 2\n"},{"name":"stdout","text":"\u001b[38;2;0;255;255;48;2;55;65;80m training will continue until epoch 12\n\u001b[0m\n\u001b[38;2;244;252;3;48;2;55;65;80m Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration\n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m11 /40     7.080   82.703   7.57995  55.882   0.00025  0.00025  accuracy     2.00     4.71  \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80m12 /40     7.046   82.973   7.52403  55.882   0.00025  0.00025  accuracy     0.33     4.43  \n\u001b[0m\n\u001b[38;2;0;255;255;48;2;55;65;80menter H to halt training or an integer for number of epochs to run then ask again\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 2\n"},{"name":"stdout","text":"\u001b[38;2;0;255;255;48;2;55;65;80m training will continue until epoch 14\n\u001b[0m\n\u001b[38;2;244;252;3;48;2;55;65;80m Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration\n\u001b[0m\n\u001b[38;2;245;170;66;48;2;55;65;80m13 /40     6.992   82.703   7.46788  61.765   0.00025  0.00013  accuracy    -0.33     4.64  \n\u001b[0m\n\u001b[38;2;245;170;66;48;2;55;65;80m14 /40     6.968   82.162   7.48267  58.824   0.00013  0.00006  accuracy    -0.98     4.74  \n\u001b[0m\n\u001b[38;2;0;255;255;48;2;55;65;80menter H to halt training or an integer for number of epochs to run then ask again\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 4\n"},{"name":"stdout","text":"\u001b[38;2;0;255;255;48;2;55;65;80m training will continue until epoch 18\n\u001b[0m\n\u001b[38;2;244;252;3;48;2;55;65;80m Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration\n\u001b[0m\n\u001b[38;2;245;170;66;48;2;55;65;80m15 /40     7.060   81.081   7.48792  58.824   0.00006  0.00003  accuracy    -2.28     4.41  \n\u001b[0m\n\u001b[38;2;245;170;66;48;2;55;65;80m16 /40     7.015   82.973   7.49696  58.824   0.00003  0.00002  accuracy     0.00     4.62  \n\u001b[0m\n\u001b[38;2;0;255;255;48;2;55;65;80m training has been halted at epoch 16 after 4 adjustments of learning rate with no improvement\n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80mTraining is completed - model is set with weights from epoch 12 \n\u001b[0m\n\u001b[38;2;0;255;0;48;2;55;65;80mtraining elapsed time was 0.0 hours,  1.0 minutes, 49.53 seconds)\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### If you wish to utilize any of these callbacks just copy the appropriate callback definition code.\n### Read the description for the callback that is supplied and see the associated demo for the\n### callback to see how it performs and produces output.\n### if you find any errors please advise of same\n### If you find this notebook of use please consider up voting - thank you","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}